{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVT fits using tensorflow for autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name=\"w\")\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name=\"b\")\n",
    "x = [[1.0, 2.0, 3.0]]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x @ w + b\n",
    "    loss = tf.reduce_mean(y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[-0.09011936, 10.3813715 ],\n",
       "        [-0.18023872, 20.762743  ],\n",
       "        [-0.2703581 , 31.144115  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.09011936, 10.3813715 ], dtype=float32)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_dw, dl_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.09011936, 10.3813715 ], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vars = {\"w\": w, \"b\": b}\n",
    "\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "grad[\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation=\"relu\")\n",
    "x = tf.constant([[1.0, 2.0, 3.0]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass\n",
    "    y = layer(x)\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad = tape.gradient(loss, layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0, shape: (3, 2)\n",
      "dense/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "    print(f\"{var.name}, shape: {g.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name=\"x0\")\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name=\"x1\", trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name=\"x2\") + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name=\"x3\")\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x**2\n",
    "\n",
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.Variable(0.0)\n",
    "x1 = tf.Variable(10.0)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(x1)\n",
    "    y0 = tf.math.sin(x0)\n",
    "    y1 = tf.nn.softplus(x1)\n",
    "    y = y0 + y1\n",
    "    ys = tf.reduce_sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx0: None\n",
      "dy/dx1: 0.9999546\n"
     ]
    }
   ],
   "source": [
    "# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n",
    "grad = tape.gradient(ys, {\"x0\": x0, \"x1\": x1})\n",
    "\n",
    "print(\"dy/dx0:\", grad[\"x0\"])\n",
    "print(\"dy/dx1:\", grad[\"x1\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "\n",
    "# Use the tape to compute the gradient of z with respect to the\n",
    "# intermediate value y.\n",
    "# dz_dy = 2 * y and y = x ** 2 = 9\n",
    "print(tape.gradient(z, y).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4. 108.]\n",
      "[2. 6.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "\n",
    "print(tape.gradient(z, x).numpy())  # [4.0, 108.0] (4 * x**3 at x = [1.0, 3.0])\n",
    "print(tape.gradient(y, x).numpy())  # [2.0, 6.0] (2 * x at x = [1.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/simon/micromamba/envs/tcpips/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/simon/micromamba/envs/tcpips/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/simon/micromamba/envs/tcpips/lib/python3.10/site-packages/tensorflow/python/ops/distributions/normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/simon/micromamba/envs/tcpips/lib/python3.10/site-packages/tensorflow/python/ops/distributions/normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 3373.111328125, Mean: 0.009999933652579784, Stddev: 1.0099999904632568\n",
      "Step 100, Loss: 1882.8646240234375, Mean: 0.7618048191070557, Stddev: 1.5001142024993896\n",
      "Step 200, Loss: 1656.1044921875, Mean: 1.2704793214797974, Stddev: 1.4409856796264648\n",
      "Step 300, Loss: 1498.5015869140625, Mean: 1.6771808862686157, Stddev: 1.2032581567764282\n",
      "Step 400, Loss: 1448.788818359375, Mean: 1.9070074558258057, Stddev: 1.0438764095306396\n",
      "Step 500, Loss: 1447.109130859375, Mean: 1.9574655294418335, Stddev: 1.0286296606063843\n",
      "Step 600, Loss: 1447.097900390625, Mean: 1.9620147943496704, Stddev: 1.0285587310791016\n",
      "Step 700, Loss: 1447.097900390625, Mean: 1.96224045753479, Stddev: 1.0285587310791016\n",
      "Step 800, Loss: 1447.097900390625, Mean: 1.962245225906372, Stddev: 1.0285587310791016\n",
      "Step 900, Loss: 1447.097900390625, Mean: 1.962245225906372, Stddev: 1.0285587310791016\n",
      "Estimated Mean: 1.962245225906372, Estimated Stddev: 1.0285587310791016\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate some synthetic data (e.g., from a normal distribution)\n",
    "true_mean = 2.0\n",
    "true_stddev = 1.0\n",
    "data = np.random.normal(true_mean, true_stddev, size=1000)\n",
    "\n",
    "# Define the parameters of the model\n",
    "mean = tf.Variable(0.0, dtype=tf.float32)\n",
    "stddev = tf.Variable(1.0, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Define the log likelihood function\n",
    "def log_likelihood(mean, stddev, data):\n",
    "    dist = tf.compat.v1.distributions.Normal(loc=mean, scale=stddev)\n",
    "    log_likelihoods = dist.log_prob(data)\n",
    "    return tf.reduce_sum(log_likelihoods)\n",
    "\n",
    "\n",
    "# Define the loss function (negative log likelihood)\n",
    "def neg_log_likelihood():\n",
    "    return -log_likelihood(mean, stddev, data)\n",
    "\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# Define the training step\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = neg_log_likelihood()\n",
    "    gradients = tape.gradient(loss, [mean, stddev])\n",
    "    optimizer.apply_gradients(zip(gradients, [mean, stddev]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for step in range(1000):\n",
    "    loss = train_step()\n",
    "    if step % 100 == 0:\n",
    "        print(\n",
    "            f\"Step {step}, Loss: {loss.numpy()}, Mean: {mean.numpy()}, Stddev: {stddev.numpy()}\"\n",
    "        )\n",
    "\n",
    "print(f\"Estimated Mean: {mean.numpy()}, Estimated Stddev: {stddev.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: inf, Loc: 0.00999993272125721, Scale: 0.9900000691413879, Concentration: -0.00999993272125721\n",
      "Step 100, Loss: 1614.80078125, Loc: 0.0017401673831045628, Scale: 1.0042678117752075, Concentration: 0.056024856865406036\n",
      "Step 200, Loss: 1614.800537109375, Loc: 0.0020912806503474712, Scale: 1.0046255588531494, Concentration: 0.055535607039928436\n",
      "Step 300, Loss: 1614.8006591796875, Loc: 0.002088126027956605, Scale: 1.0046230554580688, Concentration: 0.05553702637553215\n",
      "Step 400, Loss: 1614.800537109375, Loc: 0.0020880335941910744, Scale: 1.0046230554580688, Concentration: 0.05553735792636871\n",
      "Step 500, Loss: 1614.8006591796875, Loc: 0.0020887732971459627, Scale: 1.004623293876648, Concentration: 0.055536869913339615\n",
      "Step 600, Loss: 1614.800537109375, Loc: 0.0020880599040538073, Scale: 1.0046229362487793, Concentration: 0.055537089705467224\n",
      "Step 700, Loss: 1614.800537109375, Loc: 0.002087672706693411, Scale: 1.0046228170394897, Concentration: 0.055536944419145584\n",
      "Step 800, Loss: 1614.800537109375, Loc: 0.0020879905205219984, Scale: 1.0046228170394897, Concentration: 0.05553736910223961\n",
      "Step 900, Loss: 1614.800537109375, Loc: 0.002088371431455016, Scale: 1.0046231746673584, Concentration: 0.05553693696856499\n",
      "Step 1000, Loss: 1614.800537109375, Loc: 0.0020884692203253508, Scale: 1.0046230554580688, Concentration: 0.05553660914301872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1100, Loss: 1614.8006591796875, Loc: 0.0020882871467620134, Scale: 1.0046229362487793, Concentration: 0.055536527186632156\n",
      "Step 1200, Loss: 1614.8006591796875, Loc: 0.002088529523462057, Scale: 1.0046230554580688, Concentration: 0.055536750704050064\n",
      "Step 1300, Loss: 1614.800537109375, Loc: 0.002087698085233569, Scale: 1.0046226978302002, Concentration: 0.055537041276693344\n",
      "Step 1400, Loss: 1614.800537109375, Loc: 0.0020872456952929497, Scale: 1.004622459411621, Concentration: 0.05553755909204483\n",
      "Step 1500, Loss: 1614.8006591796875, Loc: 0.002088274573907256, Scale: 1.0046231746673584, Concentration: 0.055537011474370956\n",
      "Step 1600, Loss: 1614.8006591796875, Loc: 0.002088517416268587, Scale: 1.0046230554580688, Concentration: 0.055537231266498566\n",
      "Step 1700, Loss: 1614.800537109375, Loc: 0.0020874342881143093, Scale: 1.0046225786209106, Concentration: 0.05553752928972244\n",
      "Step 1800, Loss: 1614.8006591796875, Loc: 0.002088534412905574, Scale: 1.0046230554580688, Concentration: 0.05553622543811798\n",
      "Step 1900, Loss: 1614.8006591796875, Loc: 0.00208779308013618, Scale: 1.0046228170394897, Concentration: 0.05553669109940529\n",
      "Estimated Loc: 0.002088154200464487, Estimated Scale: 1.0046230554580688, Estimated Concentration: 0.05553679168224335\n",
      "z_star 10.0\n",
      "Step 0, Loss: inf, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 100, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 200, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 300, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 400, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 500, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 600, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 700, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 800, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 900, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 1000, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 1100, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 1200, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 1300, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 1400, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 1500, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 1600, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 1700, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 1800, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Step 1900, Loss: nan, Loc: nan, Scale: nan, Concentration: nan\n",
      "Estimated Loc: nan, Estimated Scale: nan, Estimated Concentration: nan\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "# Use the TensorFlow Probability package\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Generate some synthetic data (for demonstration purposes, using a GEV distribution)\n",
    "true_loc = 0.0\n",
    "true_scale = 1.0\n",
    "true_concentration = 0.1  # Shape parameter, also known as xi\n",
    "\n",
    "gev = tfd.GeneralizedExtremeValue(\n",
    "    loc=true_loc, scale=true_scale, concentration=true_concentration\n",
    ")\n",
    "data = gev.sample(1000).numpy()\n",
    "\n",
    "\n",
    "def fit_gev(data):\n",
    "    # Define the parameters of the model\n",
    "    loc = tf.Variable(0.0, dtype=tf.float32)\n",
    "    scale = tf.Variable(1.0, dtype=tf.float32)\n",
    "    concentration = tf.Variable(0.0, dtype=tf.float32)  # Start with zero for stability\n",
    "\n",
    "    # Define the log likelihood function\n",
    "    def log_likelihood(loc, scale, concentration, data):\n",
    "        dist = tfd.GeneralizedExtremeValue(\n",
    "            loc=loc, scale=scale, concentration=concentration\n",
    "        )\n",
    "        log_likelihoods = dist.log_prob(data)\n",
    "        return tf.reduce_sum(log_likelihoods)\n",
    "\n",
    "    # Define the loss function (negative log likelihood)\n",
    "    def neg_log_likelihood():\n",
    "        return -log_likelihood(loc, scale, concentration, data)\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    # Define the training step\n",
    "    @tf.function\n",
    "    def train_step():\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = neg_log_likelihood()\n",
    "        gradients = tape.gradient(loss, [loc, scale, concentration])\n",
    "        optimizer.apply_gradients(zip(gradients, [loc, scale, concentration]))\n",
    "        return loss\n",
    "\n",
    "    # Training loop\n",
    "    for step in range(2000):\n",
    "        loss = train_step()\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Step {step}, Loss: {loss.numpy()}, Loc: {loc.numpy()}, Scale: {scale.numpy()}, Concentration: {concentration.numpy()}\"\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"Estimated Loc: {loc.numpy()}, Estimated Scale: {scale.numpy()}, Estimated Concentration: {concentration.numpy()}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_upknown(data, z_star):\n",
    "    # Define the parameters of the model\n",
    "    # loc = tf.Variable(0.0, dtype=tf.float32)\n",
    "    print(\"z_star\", z_star)\n",
    "    scale = tf.Variable(1.0, dtype=tf.float32, constraint=tf.keras.constraints.NonNeg())\n",
    "    concentration = tf.Variable(\n",
    "        0.3, dtype=tf.float32, constraint=tf.keras.constraints.NonNeg()\n",
    "    )\n",
    "\n",
    "    # Define the log likelihood function\n",
    "    def log_likelihood(scale, concentration, data):\n",
    "        nonlocal z_star\n",
    "        dist = tfd.GeneralizedExtremeValue(\n",
    "            loc=z_star - scale/concentration, scale=scale, concentration=concentration\n",
    "        )\n",
    "        log_likelihoods = dist.log_prob(data)\n",
    "        return tf.reduce_sum(log_likelihoods)\n",
    "\n",
    "    # Define the loss function (negative log likelihood)\n",
    "    def neg_log_likelihood():\n",
    "        return -log_likelihood(scale, concentration, data)\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    # Define the training step\n",
    "    @tf.function\n",
    "    def train_step():\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = neg_log_likelihood()\n",
    "        gradients = tape.gradient(loss, [scale, concentration])\n",
    "        optimizer.apply_gradients(zip(gradients, [scale, concentration]))\n",
    "        return loss\n",
    "\n",
    "    # Training loop\n",
    "    for step in range(2000):\n",
    "        loss = train_step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Step {step}, Loss: {loss.numpy()}, Loc: {z_star - scale.numpy()/concentration.numpy()}, Scale: {scale.numpy()}, Concentration: {concentration.numpy()}\"\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        f\"Estimated Loc: {z_star - scale.numpy()/concentration.numpy()}, Estimated Scale: {scale.numpy()}, Estimated Concentration: {concentration.numpy()}\"\n",
    "    )\n",
    "\n",
    "fit_gev(data)\n",
    "fit_upknown(data, true_loc + true_scale/true_concentration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.00193095e+00, -1.84505689e+00, -1.79991305e+00, -1.78164673e+00,\n",
       "       -1.58311450e+00, -1.55446649e+00, -1.51394868e+00, -1.50024939e+00,\n",
       "       -1.49539173e+00, -1.48757899e+00, -1.44654036e+00, -1.40037417e+00,\n",
       "       -1.37084806e+00, -1.36495650e+00, -1.34764135e+00, -1.30542183e+00,\n",
       "       -1.29790020e+00, -1.26785803e+00, -1.25201881e+00, -1.24754655e+00,\n",
       "       -1.24228466e+00, -1.22549975e+00, -1.21789789e+00, -1.21132421e+00,\n",
       "       -1.20549142e+00, -1.20315683e+00, -1.20298469e+00, -1.19230843e+00,\n",
       "       -1.18968797e+00, -1.18218219e+00, -1.16179276e+00, -1.15750635e+00,\n",
       "       -1.15547407e+00, -1.15024745e+00, -1.13925087e+00, -1.12313235e+00,\n",
       "       -1.12298286e+00, -1.11856675e+00, -1.11249340e+00, -1.10949862e+00,\n",
       "       -1.10870683e+00, -1.10341585e+00, -1.09967697e+00, -1.09793282e+00,\n",
       "       -1.09141767e+00, -1.08931231e+00, -1.08854055e+00, -1.07134569e+00,\n",
       "       -1.07063842e+00, -1.07009339e+00, -1.06978297e+00, -1.06560516e+00,\n",
       "       -1.05201221e+00, -1.04963732e+00, -1.03558481e+00, -1.03465784e+00,\n",
       "       -1.03342235e+00, -1.03182125e+00, -1.01933539e+00, -1.01620591e+00,\n",
       "       -1.01253891e+00, -1.00797558e+00, -1.00457811e+00, -1.00397718e+00,\n",
       "       -1.00369990e+00, -9.99926627e-01, -9.99683201e-01, -9.96340811e-01,\n",
       "       -9.73245561e-01, -9.69524443e-01, -9.47880387e-01, -9.39427018e-01,\n",
       "       -9.35529530e-01, -9.33497667e-01, -9.29381371e-01, -9.27909434e-01,\n",
       "       -9.26494122e-01, -9.20042098e-01, -9.16821122e-01, -9.08460081e-01,\n",
       "       -9.06527460e-01, -9.01934206e-01, -8.94268215e-01, -8.91527295e-01,\n",
       "       -8.89608800e-01, -8.88844192e-01, -8.84701014e-01, -8.83095026e-01,\n",
       "       -8.82942677e-01, -8.76038074e-01, -8.70794058e-01, -8.70200336e-01,\n",
       "       -8.62112701e-01, -8.61041069e-01, -8.59142840e-01, -8.54395151e-01,\n",
       "       -8.51812720e-01, -8.47324967e-01, -8.44591677e-01, -8.42112601e-01,\n",
       "       -8.38070929e-01, -8.36355805e-01, -8.33868682e-01, -8.32619011e-01,\n",
       "       -8.29118490e-01, -8.24090540e-01, -8.23843718e-01, -8.21042359e-01,\n",
       "       -8.14792752e-01, -8.14225197e-01, -8.04481804e-01, -8.02969396e-01,\n",
       "       -7.82329381e-01, -7.79693782e-01, -7.67848313e-01, -7.61890471e-01,\n",
       "       -7.55590916e-01, -7.54150391e-01, -7.50933230e-01, -7.48881042e-01,\n",
       "       -7.44914353e-01, -7.44557500e-01, -7.42731333e-01, -7.42156267e-01,\n",
       "       -7.41794407e-01, -7.36071229e-01, -7.08919525e-01, -7.08854914e-01,\n",
       "       -7.07577407e-01, -7.04340458e-01, -6.99969292e-01, -6.98651433e-01,\n",
       "       -6.89469099e-01, -6.78887486e-01, -6.70361757e-01, -6.68840528e-01,\n",
       "       -6.66366458e-01, -6.59599900e-01, -6.53504968e-01, -6.46494865e-01,\n",
       "       -6.38058424e-01, -6.31537855e-01, -6.28114879e-01, -6.27441049e-01,\n",
       "       -6.07114255e-01, -6.06416881e-01, -6.00755155e-01, -5.86487055e-01,\n",
       "       -5.85038602e-01, -5.84114432e-01, -5.82390726e-01, -5.82094312e-01,\n",
       "       -5.80852866e-01, -5.79311013e-01, -5.77885687e-01, -5.77141881e-01,\n",
       "       -5.74767232e-01, -5.70753217e-01, -5.69445670e-01, -5.68347216e-01,\n",
       "       -5.67183971e-01, -5.63185692e-01, -5.61752856e-01, -5.59401155e-01,\n",
       "       -5.58410943e-01, -5.57717383e-01, -5.57314873e-01, -5.54280221e-01,\n",
       "       -5.49314737e-01, -5.49190581e-01, -5.47949493e-01, -5.44732273e-01,\n",
       "       -5.42608082e-01, -5.41361272e-01, -5.38922846e-01, -5.38572788e-01,\n",
       "       -5.33106267e-01, -5.30516803e-01, -5.30458391e-01, -5.29933751e-01,\n",
       "       -5.28278172e-01, -5.25296688e-01, -5.15526712e-01, -5.10207057e-01,\n",
       "       -5.09287179e-01, -5.03875792e-01, -4.99828011e-01, -4.99765754e-01,\n",
       "       -4.99310076e-01, -4.97761965e-01, -4.96694773e-01, -4.92280602e-01,\n",
       "       -4.91530180e-01, -4.86253470e-01, -4.84608233e-01, -4.83767569e-01,\n",
       "       -4.80770737e-01, -4.80383486e-01, -4.79323834e-01, -4.78715956e-01,\n",
       "       -4.77215648e-01, -4.72193837e-01, -4.72129941e-01, -4.66676652e-01,\n",
       "       -4.61510301e-01, -4.60615873e-01, -4.56993043e-01, -4.55185145e-01,\n",
       "       -4.53610688e-01, -4.43876892e-01, -4.41032618e-01, -4.39979017e-01,\n",
       "       -4.32487905e-01, -4.28905904e-01, -4.28012222e-01, -4.25281048e-01,\n",
       "       -4.25118625e-01, -4.24584150e-01, -4.18893099e-01, -4.18472290e-01,\n",
       "       -4.16227102e-01, -4.16172922e-01, -4.10919219e-01, -4.08351988e-01,\n",
       "       -4.05385673e-01, -4.05373633e-01, -4.03953254e-01, -4.03292507e-01,\n",
       "       -4.02699441e-01, -4.01376635e-01, -4.01222765e-01, -3.98858458e-01,\n",
       "       -3.98730785e-01, -3.98621976e-01, -3.95542443e-01, -3.81781310e-01,\n",
       "       -3.79666388e-01, -3.77354085e-01, -3.70546162e-01, -3.66580009e-01,\n",
       "       -3.64565730e-01, -3.62261653e-01, -3.61633897e-01, -3.58495116e-01,\n",
       "       -3.53744775e-01, -3.53574961e-01, -3.51400405e-01, -3.45351577e-01,\n",
       "       -3.41055661e-01, -3.37749422e-01, -3.33028853e-01, -3.29602778e-01,\n",
       "       -3.26652497e-01, -3.26156378e-01, -3.23620260e-01, -3.23582649e-01,\n",
       "       -3.19521874e-01, -3.18760335e-01, -3.15894365e-01, -3.12879592e-01,\n",
       "       -3.06355506e-01, -3.06147754e-01, -3.04235935e-01, -3.03621739e-01,\n",
       "       -3.02198350e-01, -2.95819372e-01, -2.95356244e-01, -2.95348674e-01,\n",
       "       -2.94480175e-01, -2.91343689e-01, -2.87435174e-01, -2.86947310e-01,\n",
       "       -2.85923332e-01, -2.82381743e-01, -2.80215979e-01, -2.79057324e-01,\n",
       "       -2.71321923e-01, -2.68970519e-01, -2.68347830e-01, -2.67742634e-01,\n",
       "       -2.66899467e-01, -2.66335160e-01, -2.57215708e-01, -2.57049352e-01,\n",
       "       -2.56628990e-01, -2.54710585e-01, -2.51518935e-01, -2.51354992e-01,\n",
       "       -2.48854011e-01, -2.46574953e-01, -2.43877500e-01, -2.39330381e-01,\n",
       "       -2.37223655e-01, -2.33853221e-01, -2.31909662e-01, -2.31050357e-01,\n",
       "       -2.30826095e-01, -2.26822853e-01, -2.24990323e-01, -2.24087745e-01,\n",
       "       -2.21452475e-01, -2.20321208e-01, -2.16198429e-01, -2.14214042e-01,\n",
       "       -2.08734870e-01, -2.06835717e-01, -1.96120590e-01, -1.92226022e-01,\n",
       "       -1.91298380e-01, -1.88036248e-01, -1.87892973e-01, -1.84888795e-01,\n",
       "       -1.84435308e-01, -1.83228180e-01, -1.67140171e-01, -1.66952908e-01,\n",
       "       -1.62974805e-01, -1.59989059e-01, -1.59081370e-01, -1.58051550e-01,\n",
       "       -1.57860845e-01, -1.57366693e-01, -1.56564131e-01, -1.54863611e-01,\n",
       "       -1.53756276e-01, -1.52754545e-01, -1.50895312e-01, -1.49960652e-01,\n",
       "       -1.44655854e-01, -1.44031078e-01, -1.41085193e-01, -1.39071256e-01,\n",
       "       -1.36401266e-01, -1.34985104e-01, -1.33449093e-01, -1.32807508e-01,\n",
       "       -1.32618025e-01, -1.29125595e-01, -1.27523705e-01, -1.23575822e-01,\n",
       "       -1.20242305e-01, -1.19531870e-01, -1.15944557e-01, -1.15317173e-01,\n",
       "       -1.12055294e-01, -1.09048367e-01, -1.05661355e-01, -9.62292179e-02,\n",
       "       -9.27983746e-02, -8.48413929e-02, -8.08245689e-02, -7.71670938e-02,\n",
       "       -7.36426711e-02, -7.21781626e-02, -7.18142688e-02, -6.84162080e-02,\n",
       "       -6.83664605e-02, -6.06643483e-02, -5.71938828e-02, -5.64573668e-02,\n",
       "       -5.23466244e-02, -5.00804074e-02, -4.84581850e-02, -4.76871505e-02,\n",
       "       -4.62466516e-02, -3.96536291e-02, -3.52278613e-02, -3.47878821e-02,\n",
       "       -3.41322161e-02, -3.40349674e-02, -3.29656899e-02, -3.15130800e-02,\n",
       "       -2.93698423e-02, -2.76485346e-02, -2.42641103e-02, -2.33282540e-02,\n",
       "       -2.16712132e-02, -1.53366439e-02, -1.45461634e-02, -1.30409291e-02,\n",
       "       -3.05733923e-03, -1.12940231e-03, -3.97356926e-04,  1.60597463e-03,\n",
       "        4.13713930e-03,  5.42101730e-03,  1.05993319e-02,  1.79095659e-02,\n",
       "        2.03631707e-02,  2.43926793e-02,  2.62628347e-02,  3.46700810e-02,\n",
       "        3.58968154e-02,  3.84109989e-02,  4.09538932e-02,  4.64315675e-02,\n",
       "        4.92945388e-02,  5.40561937e-02,  5.49708121e-02,  5.56900389e-02,\n",
       "        5.66825271e-02,  5.80583736e-02,  6.09449297e-02,  6.17873706e-02,\n",
       "        6.53805658e-02,  6.68460131e-02,  7.46533051e-02,  9.23167542e-02,\n",
       "        9.52591524e-02,  9.67224613e-02,  9.87887755e-02,  1.01812072e-01,\n",
       "        1.02718502e-01,  1.02896869e-01,  1.08575307e-01,  1.09817311e-01,\n",
       "        1.10512972e-01,  1.11194804e-01,  1.11200407e-01,  1.12757958e-01,\n",
       "        1.13570102e-01,  1.21768855e-01,  1.22242503e-01,  1.25945121e-01,\n",
       "        1.33816838e-01,  1.35002226e-01,  1.36004046e-01,  1.41803503e-01,\n",
       "        1.43160596e-01,  1.49762511e-01,  1.54827625e-01,  1.56457901e-01,\n",
       "        1.58216655e-01,  1.58967689e-01,  1.61143824e-01,  1.62870944e-01,\n",
       "        1.66806221e-01,  1.67040527e-01,  1.68793514e-01,  1.74201146e-01,\n",
       "        1.77054733e-01,  1.84057459e-01,  1.85141698e-01,  1.87306792e-01,\n",
       "        1.90978333e-01,  1.98675871e-01,  2.10571274e-01,  2.13515788e-01,\n",
       "        2.14280277e-01,  2.14448273e-01,  2.15223789e-01,  2.18656048e-01,\n",
       "        2.22851038e-01,  2.23556519e-01,  2.24226281e-01,  2.35412493e-01,\n",
       "        2.36046240e-01,  2.40668148e-01,  2.43926302e-01,  2.45895416e-01,\n",
       "        2.51495361e-01,  2.57328004e-01,  2.62184352e-01,  2.64465988e-01,\n",
       "        2.69388109e-01,  2.69951940e-01,  2.70525038e-01,  2.71157771e-01,\n",
       "        2.71465272e-01,  2.72303969e-01,  2.81676471e-01,  2.89489895e-01,\n",
       "        2.89694130e-01,  2.89709300e-01,  2.92696625e-01,  2.94957101e-01,\n",
       "        2.99882919e-01,  3.01459610e-01,  3.06660533e-01,  3.09346169e-01,\n",
       "        3.13506424e-01,  3.18569303e-01,  3.19501072e-01,  3.20956379e-01,\n",
       "        3.31570566e-01,  3.31946373e-01,  3.32123369e-01,  3.32942665e-01,\n",
       "        3.41438651e-01,  3.42650712e-01,  3.46569389e-01,  3.54185969e-01,\n",
       "        3.55884850e-01,  3.60191166e-01,  3.63564372e-01,  3.65328401e-01,\n",
       "        3.66255522e-01,  3.66381854e-01,  3.68025720e-01,  3.69624525e-01,\n",
       "        3.79108489e-01,  3.80053520e-01,  3.83633524e-01,  3.85230839e-01,\n",
       "        3.87842029e-01,  3.89414400e-01,  3.94888788e-01,  3.97348315e-01,\n",
       "        3.98738235e-01,  3.99884403e-01,  4.04025674e-01,  4.08368230e-01,\n",
       "        4.11855638e-01,  4.16287184e-01,  4.20488745e-01,  4.23915535e-01,\n",
       "        4.24641192e-01,  4.28391099e-01,  4.30610150e-01,  4.32553470e-01,\n",
       "        4.35986549e-01,  4.41291660e-01,  4.51604575e-01,  4.62690443e-01,\n",
       "        4.65932608e-01,  4.66523528e-01,  4.73158091e-01,  4.77324724e-01,\n",
       "        4.81036723e-01,  4.81206626e-01,  4.86372322e-01,  4.93756741e-01,\n",
       "        5.02212107e-01,  5.03214657e-01,  5.04580438e-01,  5.11619508e-01,\n",
       "        5.12489259e-01,  5.12612402e-01,  5.13768494e-01,  5.17595172e-01,\n",
       "        5.18214107e-01,  5.21611810e-01,  5.26418686e-01,  5.31797528e-01,\n",
       "        5.32415509e-01,  5.36223054e-01,  5.45123875e-01,  5.45383990e-01,\n",
       "        5.46242595e-01,  5.47870874e-01,  5.50683320e-01,  5.68059325e-01,\n",
       "        5.71289003e-01,  5.80441892e-01,  5.81891239e-01,  5.90746701e-01,\n",
       "        5.92699468e-01,  5.98280311e-01,  6.00783348e-01,  6.01325631e-01,\n",
       "        6.06323779e-01,  6.07206047e-01,  6.12996936e-01,  6.13667667e-01,\n",
       "        6.18054986e-01,  6.26526833e-01,  6.26719296e-01,  6.27314508e-01,\n",
       "        6.28350973e-01,  6.30583227e-01,  6.32021964e-01,  6.33374870e-01,\n",
       "        6.34117424e-01,  6.41417921e-01,  6.43588722e-01,  6.46368027e-01,\n",
       "        6.49687946e-01,  6.54599488e-01,  6.60131335e-01,  6.63369954e-01,\n",
       "        6.74120188e-01,  6.75144851e-01,  6.79467320e-01,  6.81136489e-01,\n",
       "        6.84745610e-01,  6.89128876e-01,  6.89660132e-01,  6.93180561e-01,\n",
       "        6.95072412e-01,  6.96321845e-01,  7.03082502e-01,  7.05183506e-01,\n",
       "        7.07370400e-01,  7.15677738e-01,  7.15996265e-01,  7.16474473e-01,\n",
       "        7.17271268e-01,  7.20325828e-01,  7.21641839e-01,  7.31541276e-01,\n",
       "        7.33392298e-01,  7.38330603e-01,  7.41829097e-01,  7.43898690e-01,\n",
       "        7.44479835e-01,  7.46069968e-01,  7.51057982e-01,  7.51868248e-01,\n",
       "        7.54148781e-01,  7.58472204e-01,  7.66951263e-01,  7.73343265e-01,\n",
       "        7.73946583e-01,  7.74388909e-01,  7.79163539e-01,  7.80436218e-01,\n",
       "        7.80773520e-01,  7.82673001e-01,  7.83380270e-01,  7.90773332e-01,\n",
       "        7.94169903e-01,  7.96810508e-01,  7.96942711e-01,  7.98962057e-01,\n",
       "        8.00685823e-01,  8.09465408e-01,  8.21586788e-01,  8.21712554e-01,\n",
       "        8.23937654e-01,  8.28726351e-01,  8.29862118e-01,  8.40969741e-01,\n",
       "        8.42861891e-01,  8.42890859e-01,  8.43287170e-01,  8.50123703e-01,\n",
       "        8.53658915e-01,  8.54510307e-01,  8.60602677e-01,  8.61310124e-01,\n",
       "        8.69997740e-01,  8.79233122e-01,  8.79416525e-01,  8.81336868e-01,\n",
       "        8.87096465e-01,  8.88337612e-01,  8.89407337e-01,  8.93201292e-01,\n",
       "        9.03213978e-01,  9.03353930e-01,  9.04191375e-01,  9.11274850e-01,\n",
       "        9.19630527e-01,  9.24871504e-01,  9.26156580e-01,  9.26958621e-01,\n",
       "        9.28720832e-01,  9.29027140e-01,  9.29036200e-01,  9.31484938e-01,\n",
       "        9.38035488e-01,  9.38326240e-01,  9.38887835e-01,  9.39143538e-01,\n",
       "        9.40384984e-01,  9.51074362e-01,  9.54766214e-01,  9.60342944e-01,\n",
       "        9.61263180e-01,  9.61562753e-01,  9.62286949e-01,  9.67036128e-01,\n",
       "        9.80560720e-01,  9.90562737e-01,  9.93420780e-01,  9.93970275e-01,\n",
       "        9.98517692e-01,  1.00002217e+00,  1.00044954e+00,  1.00650394e+00,\n",
       "        1.01244175e+00,  1.01379216e+00,  1.01406884e+00,  1.01789582e+00,\n",
       "        1.01826811e+00,  1.02893102e+00,  1.03996480e+00,  1.04205453e+00,\n",
       "        1.04227149e+00,  1.04270279e+00,  1.04730821e+00,  1.04987180e+00,\n",
       "        1.05612898e+00,  1.06963575e+00,  1.07080233e+00,  1.07082427e+00,\n",
       "        1.08050740e+00,  1.08214998e+00,  1.08668542e+00,  1.09160149e+00,\n",
       "        1.09266770e+00,  1.09351814e+00,  1.09969485e+00,  1.10488379e+00,\n",
       "        1.10676336e+00,  1.10814261e+00,  1.12054765e+00,  1.12147713e+00,\n",
       "        1.12406826e+00,  1.13073957e+00,  1.13076138e+00,  1.13348961e+00,\n",
       "        1.14822924e+00,  1.14871240e+00,  1.14943790e+00,  1.14963734e+00,\n",
       "        1.15509737e+00,  1.16191804e+00,  1.16354179e+00,  1.16436684e+00,\n",
       "        1.16953373e+00,  1.18543911e+00,  1.18830132e+00,  1.19537807e+00,\n",
       "        1.20306695e+00,  1.20793593e+00,  1.21430147e+00,  1.22335863e+00,\n",
       "        1.22699475e+00,  1.23230100e+00,  1.23292625e+00,  1.23555624e+00,\n",
       "        1.24830842e+00,  1.25432897e+00,  1.25492287e+00,  1.25941277e+00,\n",
       "        1.25971162e+00,  1.27301240e+00,  1.27788961e+00,  1.28396833e+00,\n",
       "        1.28427494e+00,  1.28483486e+00,  1.28844273e+00,  1.28991270e+00,\n",
       "        1.30238199e+00,  1.30475354e+00,  1.30617666e+00,  1.31249237e+00,\n",
       "        1.31736302e+00,  1.31883132e+00,  1.31940651e+00,  1.32546532e+00,\n",
       "        1.33221388e+00,  1.33709991e+00,  1.34542120e+00,  1.34691620e+00,\n",
       "        1.34883881e+00,  1.35428810e+00,  1.36497664e+00,  1.36999977e+00,\n",
       "        1.38403106e+00,  1.38731647e+00,  1.38771760e+00,  1.39478946e+00,\n",
       "        1.40072727e+00,  1.41919386e+00,  1.42262340e+00,  1.43125069e+00,\n",
       "        1.43644619e+00,  1.43937469e+00,  1.44632351e+00,  1.44910574e+00,\n",
       "        1.44984317e+00,  1.45093691e+00,  1.45137012e+00,  1.45938098e+00,\n",
       "        1.45997787e+00,  1.46033931e+00,  1.46583462e+00,  1.47157240e+00,\n",
       "        1.47256494e+00,  1.47267246e+00,  1.47652888e+00,  1.48091459e+00,\n",
       "        1.49028862e+00,  1.49380088e+00,  1.49772465e+00,  1.50232720e+00,\n",
       "        1.50652492e+00,  1.52328861e+00,  1.52337039e+00,  1.53762996e+00,\n",
       "        1.54474986e+00,  1.54489326e+00,  1.55314457e+00,  1.56272173e+00,\n",
       "        1.56394351e+00,  1.56406784e+00,  1.57004833e+00,  1.57330561e+00,\n",
       "        1.58170533e+00,  1.58308351e+00,  1.58375025e+00,  1.58959711e+00,\n",
       "        1.59368229e+00,  1.61586332e+00,  1.61630344e+00,  1.61811721e+00,\n",
       "        1.62200105e+00,  1.62344098e+00,  1.62664306e+00,  1.64353013e+00,\n",
       "        1.64944482e+00,  1.65378773e+00,  1.65422523e+00,  1.65672469e+00,\n",
       "        1.66629326e+00,  1.67737365e+00,  1.68127489e+00,  1.68588877e+00,\n",
       "        1.69038653e+00,  1.69219875e+00,  1.70668304e+00,  1.71254039e+00,\n",
       "        1.72861683e+00,  1.73484373e+00,  1.74346495e+00,  1.75431180e+00,\n",
       "        1.77115643e+00,  1.77560079e+00,  1.78822100e+00,  1.79366839e+00,\n",
       "        1.79657888e+00,  1.79698348e+00,  1.80398130e+00,  1.80657816e+00,\n",
       "        1.80976295e+00,  1.81989372e+00,  1.82429874e+00,  1.82809103e+00,\n",
       "        1.83232093e+00,  1.83754170e+00,  1.83866310e+00,  1.84596801e+00,\n",
       "        1.84662008e+00,  1.85323441e+00,  1.86977565e+00,  1.87301362e+00,\n",
       "        1.87935936e+00,  1.88535178e+00,  1.89242113e+00,  1.89278066e+00,\n",
       "        1.89647162e+00,  1.89847827e+00,  1.90241587e+00,  1.91004181e+00,\n",
       "        1.92886794e+00,  1.94081569e+00,  1.94443178e+00,  1.94732690e+00,\n",
       "        1.95181131e+00,  1.96235204e+00,  1.96364987e+00,  1.96617258e+00,\n",
       "        1.97088027e+00,  1.98245311e+00,  1.99049902e+00,  2.00636363e+00,\n",
       "        2.03068519e+00,  2.03635335e+00,  2.07302547e+00,  2.07797790e+00,\n",
       "        2.12280750e+00,  2.12378693e+00,  2.12612128e+00,  2.14471745e+00,\n",
       "        2.15128207e+00,  2.16226554e+00,  2.17910075e+00,  2.17949629e+00,\n",
       "        2.20295191e+00,  2.24988866e+00,  2.25280905e+00,  2.26049256e+00,\n",
       "        2.26609182e+00,  2.28589988e+00,  2.29768491e+00,  2.29815054e+00,\n",
       "        2.31549430e+00,  2.32302976e+00,  2.32943177e+00,  2.33049750e+00,\n",
       "        2.33322597e+00,  2.34707165e+00,  2.36895561e+00,  2.36930537e+00,\n",
       "        2.38823652e+00,  2.39255500e+00,  2.42881346e+00,  2.43622065e+00,\n",
       "        2.45397687e+00,  2.46291208e+00,  2.46540046e+00,  2.47884321e+00,\n",
       "        2.48839164e+00,  2.51072383e+00,  2.52088428e+00,  2.53259993e+00,\n",
       "        2.54542089e+00,  2.54949617e+00,  2.58409262e+00,  2.58814931e+00,\n",
       "        2.59160089e+00,  2.60898876e+00,  2.60959029e+00,  2.64603472e+00,\n",
       "        2.65731454e+00,  2.65805531e+00,  2.66387939e+00,  2.68692708e+00,\n",
       "        2.68853354e+00,  2.70030618e+00,  2.72817469e+00,  2.75207281e+00,\n",
       "        2.77491736e+00,  2.78393531e+00,  2.78846312e+00,  2.78916502e+00,\n",
       "        2.83052182e+00,  2.83209944e+00,  2.83625364e+00,  2.84450531e+00,\n",
       "        2.90043211e+00,  2.94533587e+00,  2.95693636e+00,  2.96596742e+00,\n",
       "        2.96981239e+00,  2.97370791e+00,  2.99531245e+00,  3.01778197e+00,\n",
       "        3.03687477e+00,  3.04198170e+00,  3.06921721e+00,  3.12544513e+00,\n",
       "        3.12790537e+00,  3.13187575e+00,  3.21643853e+00,  3.21862268e+00,\n",
       "        3.23452830e+00,  3.26933551e+00,  3.27135921e+00,  3.29646564e+00,\n",
       "        3.30032682e+00,  3.30328631e+00,  3.40822124e+00,  3.47025204e+00,\n",
       "        3.52260089e+00,  3.61045837e+00,  3.63387585e+00,  3.64007449e+00,\n",
       "        3.67354798e+00,  3.72498083e+00,  3.73398709e+00,  3.77717543e+00,\n",
       "        3.78179002e+00,  3.79288316e+00,  3.83739710e+00,  3.91075063e+00,\n",
       "        3.98688793e+00,  3.99557471e+00,  4.00425911e+00,  4.03014660e+00,\n",
       "        4.07652187e+00,  4.15209389e+00,  4.17610502e+00,  4.20222378e+00,\n",
       "        4.20843554e+00,  4.22574568e+00,  4.28223944e+00,  4.33010960e+00,\n",
       "        4.37108469e+00,  4.41651487e+00,  4.43096018e+00,  4.45465708e+00,\n",
       "        4.47451687e+00,  4.52209425e+00,  4.52922153e+00,  4.56164932e+00,\n",
       "        4.58750916e+00,  4.72975159e+00,  4.94366980e+00,  4.98189735e+00,\n",
       "        5.81931400e+00,  6.17737865e+00,  6.42589664e+00,  6.47169924e+00,\n",
       "        6.54773092e+00,  6.67797089e+00,  7.43688297e+00,  9.02380657e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data = np.sort(data)\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.01813937, 0.00453484, 0.04534846, 0.09976656, 0.20406796,\n",
       "        0.23581186, 0.32650882, 0.38999657, 0.38546173, 0.31290425,\n",
       "        0.31290425, 0.27209065, 0.29476483, 0.263021  , 0.21767249,\n",
       "        0.20406796, 0.16325437, 0.1677893 , 0.0725575 , 0.08162718,\n",
       "        0.07709234, 0.07709234, 0.04988328, 0.03627875, 0.02267424,\n",
       "        0.02720906, 0.02720906, 0.02720906, 0.0317439 , 0.03627875,\n",
       "        0.00453484, 0.00906969, 0.        , 0.        , 0.        ,\n",
       "        0.00453484, 0.        , 0.00453484, 0.01360453, 0.00453485,\n",
       "        0.        , 0.        , 0.00453484, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00453485]),\n",
       " array([-2.00193095, -1.78141618, -1.5609014 , -1.34038675, -1.11987197,\n",
       "        -0.8993572 , -0.67884243, -0.45832771, -0.23781295, -0.0172982 ,\n",
       "         0.20321655,  0.4237313 ,  0.64424604,  0.86476082,  1.08527553,\n",
       "         1.30579031,  1.52630508,  1.74681985,  1.96733451,  2.18784928,\n",
       "         2.40836406,  2.62887883,  2.84939361,  3.06990838,  3.29042315,\n",
       "         3.51093769,  3.73145247,  3.95196724,  4.17248201,  4.39299679,\n",
       "         4.61351156,  4.83402634,  5.05454111,  5.27505589,  5.49557066,\n",
       "         5.71608543,  5.93660021,  6.15711498,  6.37762976,  6.59814453,\n",
       "         6.81865883,  7.0391736 ,  7.25968838,  7.48020315,  7.70071793,\n",
       "         7.9212327 ,  8.14174747,  8.36226273,  8.58277702,  8.80329227,\n",
       "         9.02380657]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data, bins=50, density=True, alpha=0.5, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the TensorFlow Probability package\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Generate some synthetic data (for demonstration purposes, using a GEV distribution)\n",
    "true_loc = 0.0\n",
    "true_scale = 1.0\n",
    "true_concentration = 0.5  # Shape parameter, also known as xi\n",
    "true_z_star =  true_loc + true_scale / true_concentration \n",
    "true_z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(z, loc, scale, concentration):\n",
    "    return -tf.math.log(scale) - (1 + (1 / concentration)) * tf.math.log(1 + concentration * (z - loc) / scale) - tf.math.pow(1 + concentration * (z - loc) / scale, -1 / concentration)\n",
    "\n",
    "\n",
    "def gev_pdf(z: np.ndarray, alpha: float, beta: float, gamma: float) -> np.ndarray:\n",
    "    return (\n",
    "        1\n",
    "        / beta\n",
    "        * (1 + gamma * (z - alpha) / beta) ** (-1 / gamma - 1)\n",
    "        * np.exp(-((1 + gamma * (z - alpha) / beta) ** (-1 / gamma)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Value for attr 'T' of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128\n\t; NodeDef: {{node Log}}; Op<name=Log; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]> [Op:Log] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m zs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39mlog(gev_pdf(zs, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.1\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m, in \u001b[0;36mlog_prob\u001b[0;34m(z, loc, scale, concentration)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(z, loc, scale, concentration):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m concentration)) \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m concentration \u001b[38;5;241m*\u001b[39m (z \u001b[38;5;241m-\u001b[39m loc) \u001b[38;5;241m/\u001b[39m scale) \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m concentration \u001b[38;5;241m*\u001b[39m (z \u001b[38;5;241m-\u001b[39m loc) \u001b[38;5;241m/\u001b[39m scale, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m concentration)\n",
      "File \u001b[0;32m~/micromamba/envs/tcpips/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/micromamba/envs/tcpips/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:5624\u001b[0m, in \u001b[0;36mlog\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   5622\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   5623\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 5624\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5625\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   5626\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/tcpips/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Value for attr 'T' of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128\n\t; NodeDef: {{node Log}}; Op<name=Log; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]> [Op:Log] name: "
     ]
    }
   ],
   "source": [
    "zs = tf.constant(np.linspace(0, 100, num=10))\n",
    "np.log(gev_pdf(zs, 1, 2, 0.1))\n",
    "log_prob(zs, 1, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
