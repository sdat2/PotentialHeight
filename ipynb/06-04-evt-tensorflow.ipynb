{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVT fits using tensorflow for autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name=\"w\")\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name=\"b\")\n",
    "x = [[1.0, 2.0, 3.0]]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x @ w + b\n",
    "    loss = tf.reduce_mean(y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[-0.09011936, 10.3813715 ],\n",
       "        [-0.18023872, 20.762743  ],\n",
       "        [-0.2703581 , 31.144115  ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.09011936, 10.3813715 ], dtype=float32)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_dw, dl_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.09011936, 10.3813715 ], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vars = {\"w\": w, \"b\": b}\n",
    "\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "grad[\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation=\"relu\")\n",
    "x = tf.constant([[1.0, 2.0, 3.0]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass\n",
    "    y = layer(x)\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad = tape.gradient(loss, layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0, shape: (3, 2)\n",
      "dense/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "    print(f\"{var.name}, shape: {g.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name=\"x0\")\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name=\"x1\", trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name=\"x2\") + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name=\"x3\")\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x**2\n",
    "\n",
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.Variable(0.0)\n",
    "x1 = tf.Variable(10.0)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(x1)\n",
    "    y0 = tf.math.sin(x0)\n",
    "    y1 = tf.nn.softplus(x1)\n",
    "    y = y0 + y1\n",
    "    ys = tf.reduce_sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx0: None\n",
      "dy/dx1: 0.9999546\n"
     ]
    }
   ],
   "source": [
    "# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n",
    "grad = tape.gradient(ys, {\"x0\": x0, \"x1\": x1})\n",
    "\n",
    "print(\"dy/dx0:\", grad[\"x0\"])\n",
    "print(\"dy/dx1:\", grad[\"x1\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "\n",
    "# Use the tape to compute the gradient of z with respect to the\n",
    "# intermediate value y.\n",
    "# dz_dy = 2 * y and y = x ** 2 = 9\n",
    "print(tape.gradient(z, y).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4. 108.]\n",
      "[2. 6.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "\n",
    "print(tape.gradient(z, x).numpy())  # [4.0, 108.0] (4 * x**3 at x = [1.0, 3.0])\n",
    "print(tape.gradient(y, x).numpy())  # [2.0, 6.0] (2 * x at x = [1.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/simon/micromamba/envs/tcpips/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/simon/micromamba/envs/tcpips/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/simon/micromamba/envs/tcpips/lib/python3.10/site-packages/tensorflow/python/ops/distributions/normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/simon/micromamba/envs/tcpips/lib/python3.10/site-packages/tensorflow/python/ops/distributions/normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 3373.111328125, Mean: 0.009999933652579784, Stddev: 1.0099999904632568\n",
      "Step 100, Loss: 1882.8646240234375, Mean: 0.7618048191070557, Stddev: 1.5001142024993896\n",
      "Step 200, Loss: 1656.1044921875, Mean: 1.2704793214797974, Stddev: 1.4409856796264648\n",
      "Step 300, Loss: 1498.5015869140625, Mean: 1.6771808862686157, Stddev: 1.2032581567764282\n",
      "Step 400, Loss: 1448.788818359375, Mean: 1.9070074558258057, Stddev: 1.0438764095306396\n",
      "Step 500, Loss: 1447.109130859375, Mean: 1.9574655294418335, Stddev: 1.0286296606063843\n",
      "Step 600, Loss: 1447.097900390625, Mean: 1.9620147943496704, Stddev: 1.0285587310791016\n",
      "Step 700, Loss: 1447.097900390625, Mean: 1.96224045753479, Stddev: 1.0285587310791016\n",
      "Step 800, Loss: 1447.097900390625, Mean: 1.962245225906372, Stddev: 1.0285587310791016\n",
      "Step 900, Loss: 1447.097900390625, Mean: 1.962245225906372, Stddev: 1.0285587310791016\n",
      "Estimated Mean: 1.962245225906372, Estimated Stddev: 1.0285587310791016\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate some synthetic data (e.g., from a normal distribution)\n",
    "true_mean = 2.0\n",
    "true_stddev = 1.0\n",
    "data = np.random.normal(true_mean, true_stddev, size=1000)\n",
    "\n",
    "# Define the parameters of the model\n",
    "mean = tf.Variable(0.0, dtype=tf.float32)\n",
    "stddev = tf.Variable(1.0, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Define the log likelihood function\n",
    "def log_likelihood(mean, stddev, data):\n",
    "    dist = tf.compat.v1.distributions.Normal(loc=mean, scale=stddev)\n",
    "    log_likelihoods = dist.log_prob(data)\n",
    "    return tf.reduce_sum(log_likelihoods)\n",
    "\n",
    "\n",
    "# Define the loss function (negative log likelihood)\n",
    "def neg_log_likelihood():\n",
    "    return -log_likelihood(mean, stddev, data)\n",
    "\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# Define the training step\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = neg_log_likelihood()\n",
    "    gradients = tape.gradient(loss, [mean, stddev])\n",
    "    optimizer.apply_gradients(zip(gradients, [mean, stddev]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for step in range(1000):\n",
    "    loss = train_step()\n",
    "    if step % 100 == 0:\n",
    "        print(\n",
    "            f\"Step {step}, Loss: {loss.numpy()}, Mean: {mean.numpy()}, Stddev: {stddev.numpy()}\"\n",
    "        )\n",
    "\n",
    "print(f\"Estimated Mean: {mean.numpy()}, Estimated Stddev: {stddev.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: inf, Loc: 0.00999993272125721, Scale: 0.9900000691413879, Concentration: -0.009999931789934635\n",
      "Step 100, Loss: 1620.2271728515625, Loc: -0.026037784293293953, Scale: 0.9806874394416809, Concentration: 0.10872902721166611\n",
      "Step 200, Loss: 1620.2271728515625, Loc: -0.025770466774702072, Scale: 0.9806209802627563, Concentration: 0.1084568127989769\n",
      "Step 300, Loss: 1620.227294921875, Loc: -0.025771239772439003, Scale: 0.9806195497512817, Concentration: 0.1084568127989769\n",
      "Step 400, Loss: 1620.227294921875, Loc: -0.02577112428843975, Scale: 0.9806195497512817, Concentration: 0.10845671594142914\n",
      "Step 500, Loss: 1620.227294921875, Loc: -0.025771521031856537, Scale: 0.9806194305419922, Concentration: 0.1084570363163948\n",
      "Step 600, Loss: 1620.227294921875, Loc: -0.025771308690309525, Scale: 0.9806194305419922, Concentration: 0.10845670849084854\n",
      "Step 700, Loss: 1620.2271728515625, Loc: -0.025771496817469597, Scale: 0.9806194305419922, Concentration: 0.10845702886581421\n",
      "Step 800, Loss: 1620.227294921875, Loc: -0.025771260261535645, Scale: 0.980619490146637, Concentration: 0.10845673829317093\n",
      "Step 900, Loss: 1620.227294921875, Loc: -0.02577122673392296, Scale: 0.9806194305419922, Concentration: 0.1084565594792366\n",
      "Step 1000, Loss: 1620.227294921875, Loc: -0.025770941749215126, Scale: 0.9806196093559265, Concentration: 0.10845625400543213\n",
      "Step 1100, Loss: 1620.227294921875, Loc: -0.025771232321858406, Scale: 0.980619490146637, Concentration: 0.10845665633678436\n",
      "Step 1200, Loss: 1620.227294921875, Loc: -0.025771187618374825, Scale: 0.980619490146637, Concentration: 0.1084565594792366\n",
      "Step 1300, Loss: 1620.227294921875, Loc: -0.025771182030439377, Scale: 0.9806196689605713, Concentration: 0.10845699161291122\n",
      "Step 1400, Loss: 1620.2271728515625, Loc: -0.025771470740437508, Scale: 0.9806194305419922, Concentration: 0.10845693945884705\n",
      "Step 1500, Loss: 1620.227294921875, Loc: -0.025770902633666992, Scale: 0.9806197285652161, Concentration: 0.10845629125833511\n",
      "Step 1600, Loss: 1620.227294921875, Loc: -0.025771094486117363, Scale: 0.980619490146637, Concentration: 0.10845642536878586\n",
      "Step 1700, Loss: 1620.227294921875, Loc: -0.025771424174308777, Scale: 0.980619490146637, Concentration: 0.10845691710710526\n",
      "Step 1800, Loss: 1620.22705078125, Loc: -0.025771232321858406, Scale: 0.9806196093559265, Concentration: 0.10845663398504257\n",
      "Step 1900, Loss: 1620.227294921875, Loc: -0.02577134221792221, Scale: 0.9806194305419922, Concentration: 0.10845699906349182\n",
      "Estimated Loc: -0.025771068409085274, Estimated Scale: 0.9806195497512817, Estimated Concentration: 0.10845614969730377\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "# Use the TensorFlow Probability package\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Generate some synthetic data (for demonstration purposes, using a GEV distribution)\n",
    "true_loc = 0.0\n",
    "true_scale = 1.0\n",
    "true_concentration = 0.1  # Shape parameter, also known as xi\n",
    "\n",
    "gev = tfd.GeneralizedExtremeValue(\n",
    "    loc=true_loc, scale=true_scale, concentration=true_concentration\n",
    ")\n",
    "data = gev.sample(1000).numpy()\n",
    "\n",
    "# Define the parameters of the model\n",
    "loc = tf.Variable(0.0, dtype=tf.float32)\n",
    "scale = tf.Variable(1.0, dtype=tf.float32)\n",
    "concentration = tf.Variable(0.0, dtype=tf.float32)  # Start with zero for stability\n",
    "\n",
    "\n",
    "# Define the log likelihood function\n",
    "def log_likelihood(loc, scale, concentration, data):\n",
    "    dist = tfd.GeneralizedExtremeValue(\n",
    "        loc=loc, scale=scale, concentration=concentration\n",
    "    )\n",
    "    log_likelihoods = dist.log_prob(data)\n",
    "    return tf.reduce_sum(log_likelihoods)\n",
    "\n",
    "\n",
    "# Define the loss function (negative log likelihood)\n",
    "def neg_log_likelihood():\n",
    "    return -log_likelihood(loc, scale, concentration, data)\n",
    "\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# Define the training step\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = neg_log_likelihood()\n",
    "    gradients = tape.gradient(loss, [loc, scale, concentration])\n",
    "    optimizer.apply_gradients(zip(gradients, [loc, scale, concentration]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for step in range(2000):\n",
    "    loss = train_step()\n",
    "    if step % 100 == 0:\n",
    "        print(\n",
    "            f\"Step {step}, Loss: {loss.numpy()}, Loc: {loc.numpy()}, Scale: {scale.numpy()}, Concentration: {concentration.numpy()}\"\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"Estimated Loc: {loc.numpy()}, Estimated Scale: {scale.numpy()}, Estimated Concentration: {concentration.numpy()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.02123572, 0.06724644, 0.16634646, 0.30791792, 0.27606432,\n",
       "        0.4176358 , 0.34331079, 0.35392865, 0.29376075, 0.23359296,\n",
       "        0.19820004, 0.1840429 , 0.15572854, 0.06724647, 0.09202141,\n",
       "        0.08494291, 0.05308927, 0.04601074, 0.01769642, 0.02123573,\n",
       "        0.04247146, 0.01769641, 0.01415715, 0.01061786, 0.00353929,\n",
       "        0.01769641, 0.00353929, 0.00353929, 0.        , 0.        ,\n",
       "        0.00353929, 0.01061786, 0.00353928, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00353928,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00353928]),\n",
       " array([-1.76652193, -1.48397911, -1.20143628, -0.91889346, -0.63635063,\n",
       "        -0.35380778, -0.07126495,  0.21127787,  0.4938207 ,  0.77636355,\n",
       "         1.05890632,  1.34144914,  1.62399197,  1.90653491,  2.18907762,\n",
       "         2.47162056,  2.75416327,  3.03670621,  3.31924891,  3.60179186,\n",
       "         3.88433456,  4.16687727,  4.44942045,  4.73196316,  5.01450586,\n",
       "         5.29704857,  5.57959175,  5.86213446,  6.14467716,  6.42722034,\n",
       "         6.70976305,  6.99230576,  7.27484846,  7.55739164,  7.83993435,\n",
       "         8.12247753,  8.40501976,  8.68756294,  8.97010517,  9.25264835,\n",
       "         9.53519154,  9.81773376, 10.10027695, 10.38282013, 10.66536236,\n",
       "        10.94790554, 11.23044777, 11.51299095, 11.79553413, 12.07807636,\n",
       "        12.36061954]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data, bins=50, density=True, alpha=0.5, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the TensorFlow Probability package\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Generate some synthetic data (for demonstration purposes, using a GEV distribution)\n",
    "true_loc = 0.0\n",
    "true_scale = 1.0\n",
    "true_concentration = 0.5  # Shape parameter, also known as xi\n",
    "true_z_star = true_loc * true_scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
